{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"pgmemory","text":"<p>Opinionated multi-user agent memory on PostgreSQL + pgvector.</p> <p>One table. Hybrid search. Lifecycle management. Any framework.</p>"},{"location":"#why-pgmemory","title":"Why pgmemory?","text":"<p>Every agent framework re-invents memory. They all need the same thing: store what the agent learned about a user, find it later, let old stuff fade. The database part is always the same \u2014 it's Postgres with vectors.</p> <p>pgmemory is that database part, extracted into a standalone library. Use it with ADK, LangChain, CrewAI, Semantic Kernel, or plain Python. The core has zero framework dependencies.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Single table \u2014 one well-indexed PostgreSQL table handles everything: categories, importance, temporal validity, provenance, and hybrid search</li> <li>Hybrid search \u2014 combines semantic similarity (pgvector), keyword matching (tsvector), and recency decay into a single weighted score</li> <li>Memory lifecycle \u2014 importance levels, temporal validity, promote/expire/decay operations, and automatic conflict resolution via <code>supersede()</code></li> <li>Framework-agnostic \u2014 core library has no framework deps. Thin adapters for Google ADK and LangChain included</li> <li>Multi-tenant \u2014 scoped by <code>app_name</code> + <code>user_id</code> out of the box</li> <li>Provenance \u2014 every memory tracks where it came from: session, event, timestamp, role</li> </ul>"},{"location":"#quick-install","title":"Quick install","text":"<pre><code>pip install pgmemory\n</code></pre> <p>With an embedding provider:</p> <pre><code>pip install pgmemory[vertex]   # Google Vertex AI\npip install pgmemory[ollama]   # Local Ollama\npip install pgmemory[openai]   # OpenAI\n</code></pre>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from pgmemory import MemoryStore, Category, SearchQuery, OllamaEmbeddingProvider\n\nstore = MemoryStore(\n    \"postgresql+asyncpg://user:pass@localhost/mydb\",\n    OllamaEmbeddingProvider(),\n)\nawait store.init()\n\n# Store a memory\nawait store.add(\"my_app\", \"user_123\", \"Prefers dark mode and compact layouts\",\n                category=Category.PREFERENCE, importance=3)\n\n# Search (hybrid: keyword + semantic + recency)\nresults = await store.search(SearchQuery(\n    app_name=\"my_app\",\n    user_id=\"user_123\",\n    text=\"UI preferences\",\n))\n\nfor r in results:\n    print(f\"[{r.memory.category.value}] {r.text} (score={r.combined_score})\")\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Getting Started \u2014 prerequisites, install, and a full working example</li> <li>Concepts \u2014 categories, hybrid search, lifecycle, conflict resolution</li> <li>API Reference \u2014 complete API documentation</li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#categories","title":"Categories","text":"<p>Every memory gets exactly one category. These are based on the Memori model and cover the types of information agents typically need to remember.</p> Category Enum When to use Fact <code>Category.FACT</code> Technical info, data points, definitions \u2014 \"Works at Acme Corp\" Preference <code>Category.PREFERENCE</code> Likes, dislikes, personal choices \u2014 \"Prefers dark mode\" Skill <code>Category.SKILL</code> Competencies, learning progress \u2014 \"Knows Python and SQL\" Context <code>Category.CONTEXT</code> Project details, current situation \u2014 \"Working on Q4 migration\" Rule <code>Category.RULE</code> Constraints, policies, guidelines \u2014 \"Never deploy on Fridays\" Event <code>Category.EVENT</code> Something that happened (episodic) \u2014 \"Had a meeting with PM\" General <code>Category.GENERAL</code> Uncategorised or raw content \u2014 the catch-all <p>Filter searches by category to reduce noise:</p> <pre><code>results = await store.search(SearchQuery(\n    app_name=\"my_app\",\n    user_id=\"user_1\",\n    text=\"what does the user like?\",\n    categories=[Category.PREFERENCE],\n))\n</code></pre>"},{"location":"concepts/#hybrid-search","title":"Hybrid search","text":"<p>Every search combines three signals into a single score:</p> Signal Method What it catches Semantic pgvector cosine similarity \"UI preferences\" finds \"likes dark mode\" Keyword PostgreSQL <code>ts_rank</code> + <code>tsvector</code> Exact terms, names, codes Recency Time-decay function Recent memories rank higher"},{"location":"concepts/#scoring-formula","title":"Scoring formula","text":"<pre><code>combined = (w_sim \u00d7 cosine_similarity)\n         + (w_kw  \u00d7 ts_rank)\n         + (w_rec \u00d7 recency_decay)\n</code></pre> <p>Where <code>recency_decay = 1 / (1 + days_old / 30)</code></p>"},{"location":"concepts/#default-weights","title":"Default weights","text":"Weight Default Description <code>weight_similarity</code> 0.6 Semantic similarity via pgvector <code>weight_keyword</code> 0.25 Full-text keyword matching <code>weight_recency</code> 0.15 Time-decay bonus for recent memories"},{"location":"concepts/#tuning-weights","title":"Tuning weights","text":"<p>Adjust per query to match your use case:</p> <pre><code># Lean heavily into semantic similarity\nSearchQuery(\n    ...,\n    weight_similarity=0.8,\n    weight_keyword=0.15,\n    weight_recency=0.05,\n)\n\n# Prioritise exact keyword matches\nSearchQuery(\n    ...,\n    weight_similarity=0.4,\n    weight_keyword=0.5,\n    weight_recency=0.1,\n)\n</code></pre>"},{"location":"concepts/#embedding-enrichment","title":"Embedding enrichment","text":"<p>By default, pgmemory prepends the memory's category to the text before generating embeddings. For example, <code>\"Never store passwords in plaintext\"</code> with category <code>RULE</code> becomes <code>\"rule: Never store passwords in plaintext\"</code> for embedding purposes (the stored text is unchanged).</p> <p>This gives the embedding model category context, which dramatically improves search quality \u2014 queries like <code>\"what are the rules\"</code> correctly surface rule-category memories that would otherwise rank lower.</p> <p>To disable enrichment:</p> <pre><code>store = MemoryStore(\n    \"postgresql+asyncpg://...\",\n    embedder,\n    enrich_embeddings=False,\n)\n</code></pre>"},{"location":"concepts/#memory-lifecycle","title":"Memory lifecycle","text":""},{"location":"concepts/#importance","title":"Importance","text":"<p>Memories have an importance level from 1 (low) to 5 (critical). Higher importance means the memory survives decay longer.</p> <ul> <li>Default importance is 1</li> <li>Call <code>store.promote(memory_id)</code> to bump importance and clear any expiry</li> <li>Filter by minimum importance in searches with <code>min_importance</code></li> </ul>"},{"location":"concepts/#temporal-validity","title":"Temporal validity","text":"<p>Memories can have a time window during which they're valid:</p> <ul> <li><code>valid_from</code> \u2014 when the memory becomes active (defaults to now)</li> <li><code>valid_until</code> \u2014 when the memory expires (<code>None</code> = never)</li> </ul> <p>Expired memories are automatically excluded from search results and cleaned up by <code>decay()</code>.</p> <pre><code>from datetime import datetime, timedelta, timezone\n\n# Memory that expires in 30 days\nawait store.add(\"my_app\", \"user_1\", \"User is on parental leave\",\n                category=Category.CONTEXT,\n                valid_until=datetime.now(timezone.utc) + timedelta(days=30))\n</code></pre>"},{"location":"concepts/#promote-expire-decay","title":"Promote, expire, decay","text":"Operation What it does <code>store.promote(id)</code> Bump importance by 1, clear <code>valid_until</code>, update <code>last_accessed</code> <code>store.expire(id)</code> Set <code>valid_until</code> to now, log reason in metadata <code>store.decay()</code> Hard-delete all memories past their <code>valid_until</code> <code>store.soft_expire_stale()</code> Set <code>valid_until</code> on old, low-importance memories that never expire"},{"location":"concepts/#conflict-resolution","title":"Conflict resolution","text":"<p>When you learn something new that contradicts an existing memory, use <code>supersede()</code>:</p> <pre><code>new_id, superseded = await store.supersede(\n    \"my_app\", \"user_123\",\n    \"Now works at Dash Corp\",       # new fact\n    Category.FACT,                  # same category\n)\n# If \"Works at Acme Corp\" exists with similarity &gt; 0.85,\n# it gets soft-expired and the new memory replaces it.\n</code></pre>"},{"location":"concepts/#how-it-works","title":"How it works","text":"<ol> <li><code>find_conflicts()</code> embeds the new text and searches for active memories in the same category with cosine similarity above the threshold (default: 0.85)</li> <li>Each conflict is soft-expired with reason <code>\"superseded (sim=X.XX)\"</code> \u2014 the old memory is preserved for audit</li> <li>The new memory is inserted</li> </ol>"},{"location":"concepts/#threshold-tuning","title":"Threshold tuning","text":"<ul> <li>0.85 (default) \u2014 only very similar memories are treated as conflicts</li> <li>0.7 \u2014 more aggressive, catches broader semantic overlaps</li> <li>0.95 \u2014 conservative, only near-duplicates</li> </ul> <pre><code>new_id, superseded = await store.supersede(\n    \"my_app\", \"user_1\", \"...\", Category.FACT,\n    threshold=0.7,  # more aggressive conflict detection\n)\n</code></pre>"},{"location":"concepts/#multi-tenancy","title":"Multi-tenancy","text":"<p>Every memory is scoped by two dimensions:</p> <ul> <li><code>app_name</code> \u2014 isolates different applications sharing the same database</li> <li><code>user_id</code> \u2014 isolates different users within an application</li> </ul> <p>All operations (search, add, lifecycle, admin) require these scoping parameters. There is no cross-user or cross-app leakage.</p>"},{"location":"concepts/#provenance","title":"Provenance","text":"<p>Every memory records where it came from:</p> Field Purpose <code>source_session_id</code> Which conversation produced this memory <code>source_event_id</code> Which specific message <code>source_event_timestamp</code> When that message happened <code>source_role</code> Who said it: <code>user</code>, <code>assistant</code>, <code>system</code> <p>This lets you trace any memory back to its origin. The ADK adapter populates these automatically from session events.</p>"},{"location":"concepts/#schema","title":"Schema","text":"<p>pgmemory uses a single table with well-chosen indexes:</p> <pre><code>memory\n\u251c\u2500\u2500 id                      SERIAL PK\n\u251c\u2500\u2500 app_name                TEXT           \u2500\u2500 multi-app isolation\n\u251c\u2500\u2500 user_id                 TEXT           \u2500\u2500 per-user scoping\n\u2502\n\u251c\u2500\u2500 content                 TEXT           \u2500\u2500 the memory text\n\u251c\u2500\u2500 content_embedding       VECTOR(n)      \u2500\u2500 cosine similarity search\n\u251c\u2500\u2500 content_tsv             TSVECTOR       \u2500\u2500 generated, for keyword search\n\u2502\n\u251c\u2500\u2500 category                TEXT           \u2500\u2500 fact/preference/skill/context/rule/event/general\n\u251c\u2500\u2500 importance              INT (1-5)      \u2500\u2500 higher = survives decay longer\n\u2502\n\u251c\u2500\u2500 created_at              TIMESTAMPTZ\n\u251c\u2500\u2500 valid_from              TIMESTAMPTZ    \u2500\u2500 when this became true\n\u251c\u2500\u2500 valid_until             TIMESTAMPTZ    \u2500\u2500 NULL = never expires\n\u251c\u2500\u2500 last_accessed           TIMESTAMPTZ    \u2500\u2500 updated on search retrieval\n\u2502\n\u251c\u2500\u2500 source_session_id       TEXT           \u2500\u2500 which conversation\n\u251c\u2500\u2500 source_event_id         TEXT           \u2500\u2500 which message\n\u251c\u2500\u2500 source_event_timestamp  TIMESTAMPTZ    \u2500\u2500 when that message happened\n\u251c\u2500\u2500 source_role             TEXT           \u2500\u2500 user / assistant / system\n\u2502\n\u2514\u2500\u2500 metadata                JSONB          \u2500\u2500 your extensible data\n</code></pre> <p>Indexes: <code>(app_name, user_id)</code>, <code>(app_name, user_id, category)</code>, <code>importance</code>, <code>created_at</code>, <code>valid_until</code>, GIN on tsvector, HNSW on embedding.</p>"},{"location":"embedding-providers/","title":"Embedding Providers","text":"<p>pgmemory needs vectors to power semantic search. How you generate them is your business \u2014 pick a built-in provider or write your own.</p>"},{"location":"embedding-providers/#google-vertex-ai","title":"Google Vertex AI","text":"<pre><code>pip install pgmemory[vertex]\n</code></pre> <pre><code>from pgmemory import VertexEmbeddingProvider\n\nembedder = VertexEmbeddingProvider(\n    model=\"text-embedding-004\",   # default\n    task_type=\"RETRIEVAL_DOCUMENT\",  # default\n    dimensionality=768,              # default\n)\n</code></pre> Parameter Default Description <code>model</code> <code>\"text-embedding-004\"</code> Vertex AI embedding model name <code>task_type</code> <code>\"RETRIEVAL_DOCUMENT\"</code> Task type hint for the model <code>dimensionality</code> <code>768</code> Output vector dimensions <p>Note</p> <p>Requires Google Cloud credentials configured (ADC, service account, etc).</p>"},{"location":"embedding-providers/#ollama-local","title":"Ollama (local)","text":"<pre><code>pip install pgmemory[ollama]\n</code></pre> <pre><code>from pgmemory import OllamaEmbeddingProvider\n\nembedder = OllamaEmbeddingProvider(\n    model=\"nomic-embed-text\",  # default\n    dimensionality=768,        # default\n    host=None,                 # default (localhost:11434)\n)\n</code></pre> Parameter Default Description <code>model</code> <code>\"nomic-embed-text\"</code> Ollama model name <code>dimensionality</code> <code>768</code> Output vector dimensions <code>host</code> <code>None</code> Ollama server URL (defaults to <code>localhost:11434</code>) <p>Tip</p> <p>Pull the model first: <code>ollama pull nomic-embed-text</code></p>"},{"location":"embedding-providers/#openai","title":"OpenAI","text":"<pre><code>pip install pgmemory[openai]\n</code></pre> <pre><code>from pgmemory import OpenAIEmbeddingProvider\n\nembedder = OpenAIEmbeddingProvider(\n    model=\"text-embedding-3-small\",  # default\n    dimensionality=1536,             # default\n    api_key=None,                    # default (uses OPENAI_API_KEY env var)\n    base_url=None,                   # default (override for Azure/proxies)\n)\n</code></pre> Parameter Default Description <code>model</code> <code>\"text-embedding-3-small\"</code> OpenAI embedding model name <code>dimensionality</code> <code>1536</code> Output vector dimensions <code>api_key</code> <code>None</code> API key (falls back to <code>OPENAI_API_KEY</code> env var) <code>base_url</code> <code>None</code> Override for Azure OpenAI or compatible APIs"},{"location":"embedding-providers/#custom-provider","title":"Custom provider","text":"<p>Implement the <code>EmbeddingProvider</code> abstract class:</p> <pre><code>from pgmemory import EmbeddingProvider\n\nclass MyEmbedder(EmbeddingProvider):\n    @property\n    def dimensions(self) -&gt; int:\n        return 384\n\n    async def embed(self, texts: Sequence[str]) -&gt; list[list[float]]:\n        # Return one vector per input text\n        return [my_model.encode(t) for t in texts]\n</code></pre> <p>Two things to implement:</p> <ol> <li><code>dimensions</code> (property) \u2014 return the vector size your model produces</li> <li><code>embed(texts)</code> (async method) \u2014 take a list of strings, return a list of float vectors</li> </ol> <p>Then pass it to <code>MemoryStore</code>:</p> <pre><code>store = MemoryStore(\n    \"postgresql+asyncpg://...\",\n    MyEmbedder(),\n)\n</code></pre> <p>Warning</p> <p>The <code>dimensions</code> value must match the actual vector size returned by <code>embed()</code>. pgvector will reject mismatched vectors.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>PostgreSQL with the pgvector extension</li> </ul>"},{"location":"getting-started/#start-postgresql-with-pgvector","title":"Start PostgreSQL with pgvector","text":"<p>The easiest way to get a pgvector-enabled Postgres:</p> <pre><code>docker run -e POSTGRES_USER=mem_user \\\n  -e POSTGRES_PASSWORD=password \\\n  -e POSTGRES_DB=mem_db \\\n  --name pgmemory \\\n  -p 5432:5432 \\\n  -d ankane/pgvector\n</code></pre>"},{"location":"getting-started/#installation","title":"Installation","text":"Core onlyWith Vertex AIWith OllamaWith OpenAIWith Google ADKEverything <pre><code>pip install pgmemory\n</code></pre> <pre><code>pip install pgmemory[vertex]\n</code></pre> <pre><code>pip install pgmemory[ollama]\n</code></pre> <pre><code>pip install pgmemory[openai]\n</code></pre> <pre><code>pip install pgmemory[adk]\n</code></pre> <pre><code>pip install pgmemory[all]\n</code></pre>"},{"location":"getting-started/#full-working-example","title":"Full working example","text":"<pre><code>import asyncio\nfrom datetime import datetime, timedelta, timezone\nfrom pgmemory import MemoryStore, Category, SearchQuery, OllamaEmbeddingProvider\n\nasync def main():\n    # 1. Initialize the store\n    store = MemoryStore(\n        \"postgresql+asyncpg://mem_user:password@localhost/mem_db\",\n        OllamaEmbeddingProvider(),  # or VertexEmbeddingProvider(), OpenAIEmbeddingProvider()\n    )\n    await store.init()\n\n    # 2. Add memories\n    pref_id = await store.add(\n        \"my_app\", \"user_123\",\n        \"Prefers dark mode and compact layouts\",\n        category=Category.PREFERENCE,\n        importance=3,\n    )\n\n    fact_id = await store.add(\n        \"my_app\", \"user_123\",\n        \"Works at Acme Corp as a data engineer\",\n        category=Category.FACT,\n        importance=2,\n        source_session_id=\"sess_abc\",\n        metadata={\"confidence\": 0.95},\n    )\n\n    # 3. Search (hybrid: keyword + semantic + recency)\n    results = await store.search(SearchQuery(\n        app_name=\"my_app\",\n        user_id=\"user_123\",\n        text=\"UI preferences\",\n    ))\n\n    for r in results:\n        print(f\"[{r.memory.category.value}] {r.text} (score={r.combined_score})\")\n\n    # 4. Lifecycle operations\n    await store.promote(pref_id)              # bump importance, prevent decay\n    await store.expire(fact_id, reason=\"outdated\")  # soft-delete with audit trail\n    await store.decay()                       # hard-delete expired memories\n\n    # 5. Conflict resolution\n    new_id, superseded = await store.supersede(\n        \"my_app\", \"user_123\",\n        \"Now works at Dash Corp\",\n        Category.FACT,\n    )\n    print(f\"New memory {new_id}, superseded: {superseded}\")\n\n    # 6. Admin\n    users = await store.list_users(\"my_app\")\n    count = await store.count(app_name=\"my_app\")\n    print(f\"Users: {users}, Total memories: {count}\")\n\n    await store.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#project-structure","title":"Project structure","text":"<pre><code>pgmemory/\n\u251c\u2500\u2500 src/pgmemory/\n\u2502   \u251c\u2500\u2500 __init__.py          # public API (MemoryStore, Category, etc.)\n\u2502   \u251c\u2500\u2500 types.py             # Memory, SearchResult, SearchQuery, Category\n\u2502   \u251c\u2500\u2500 models.py            # single-table SQLAlchemy ORM\n\u2502   \u251c\u2500\u2500 embeddings.py        # pluggable providers (Vertex, Ollama, OpenAI)\n\u2502   \u251c\u2500\u2500 store.py             # MemoryStore \u2014 the core\n\u2502   \u2514\u2500\u2500 adapters/\n\u2502       \u251c\u2500\u2500 adk.py           # Google ADK BaseMemoryService + tools\n\u2502       \u2514\u2500\u2500 langchain.py     # LangChain memory + tools\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 helpers.py           # FakeEmbeddingProvider\n    \u251c\u2500\u2500 test_unit.py         # no DB needed\n    \u2514\u2500\u2500 test_integration.py  # testcontainers pgvector\n</code></pre>"},{"location":"adapters/adk/","title":"Google ADK Adapter","text":"<p>pgmemory integrates with Google's Agent Development Kit (ADK) as both a memory service and a set of agent tools.</p>"},{"location":"adapters/adk/#installation","title":"Installation","text":"<pre><code>pip install pgmemory[adk]\n</code></pre> <p>This installs <code>google-adk</code> and <code>google-genai</code> alongside pgmemory.</p>"},{"location":"adapters/adk/#setup","title":"Setup","text":"<pre><code>from pgmemory import MemoryStore, VertexEmbeddingProvider\nfrom pgmemory.adapters.adk import ADKMemoryService, build_adk_tools\nfrom google.adk.agents import LlmAgent\nfrom google.adk.runners import Runner\nfrom google.adk.tools import preload_memory, load_memory\n\n# 1. Create the store\nstore = MemoryStore(\n    \"postgresql+asyncpg://user:pass@localhost/mydb\",\n    VertexEmbeddingProvider(),\n)\n\n# 2. Wrap as ADK memory service\nmemory_service = ADKMemoryService(store)\n\n# 3. Build agent tools\ntools = build_adk_tools(store)\n\n# 4. Wire into your agent\nagent = LlmAgent(\n    name=\"my_agent\",\n    model=\"gemini-2.0-flash\",\n    tools=[preload_memory, load_memory, *tools],\n)\n\n# 5. Create runner with memory service\nrunner = Runner(\n    agent=agent,\n    app_name=\"my_app\",\n    memory_service=memory_service,\n)\n</code></pre>"},{"location":"adapters/adk/#adkmemoryservice","title":"ADKMemoryService","text":"<p><code>ADKMemoryService</code> implements ADK's <code>BaseMemoryService</code> interface:</p> <ul> <li><code>add_session_to_memory(session)</code> \u2014 extracts text from session events, preserves provenance (session ID, event ID, timestamp, role), and batch-inserts into pgmemory</li> <li><code>search_memory(app_name, user_id, query)</code> \u2014 runs hybrid search and returns results as ADK <code>SearchMemoryResponse</code></li> </ul> <pre><code>memory_service = ADKMemoryService(\n    store,\n    search_top_k=10,        # max results per search (default: 10)\n    search_threshold=0.4,   # minimum similarity (default: 0.4)\n)\n</code></pre>"},{"location":"adapters/adk/#agent-tools","title":"Agent tools","text":"<p><code>build_adk_tools(store)</code> returns four <code>FunctionTool</code> instances:</p> Tool Description <code>commit_session_to_memory</code> Save the current conversation to long-term memory <code>remember_fact</code> Store a specific fact with category, importance, and optional expiry. Automatically runs conflict resolution via <code>supersede()</code> <code>forget_memory</code> Expire a specific memory by ID <code>reinforce_memory</code> Increase importance of a memory to prevent decay"},{"location":"adapters/adk/#remember_fact-parameters","title":"remember_fact parameters","text":"Parameter Type Default Description <code>fact</code> <code>str</code> required The information to remember <code>category</code> <code>str</code> <code>\"fact\"</code> One of: fact, preference, skill, context, rule, event <code>importance</code> <code>int</code> <code>1</code> 1 (low) to 5 (critical) <code>expires_in_days</code> <code>int \\| None</code> <code>None</code> Days until expiry. Omit for permanent"},{"location":"adapters/adk/#full-example","title":"Full example","text":"<pre><code>import asyncio\nfrom pgmemory import MemoryStore, VertexEmbeddingProvider\nfrom pgmemory.adapters.adk import ADKMemoryService, build_adk_tools\nfrom google.adk.agents import LlmAgent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import preload_memory, load_memory\nfrom google.genai import types\n\nasync def main():\n    store = MemoryStore(\n        \"postgresql+asyncpg://mem_user:password@localhost/mem_db\",\n        VertexEmbeddingProvider(),\n    )\n    await store.init()\n\n    memory_service = ADKMemoryService(store)\n    session_service = InMemorySessionService()\n\n    agent = LlmAgent(\n        name=\"assistant\",\n        model=\"gemini-2.0-flash\",\n        instruction=\"You are a helpful assistant with long-term memory.\",\n        tools=[preload_memory, load_memory, *build_adk_tools(store)],\n    )\n\n    runner = Runner(\n        agent=agent,\n        app_name=\"my_app\",\n        session_service=session_service,\n        memory_service=memory_service,\n    )\n\n    session = await session_service.create_session(\n        app_name=\"my_app\", user_id=\"user_123\"\n    )\n\n    message = types.Content(\n        parts=[types.Part(text=\"Remember that I prefer Python over Java\")],\n        role=\"user\",\n    )\n\n    async for event in runner.run_async(\n        session_id=session.id, user_id=\"user_123\", new_message=message\n    ):\n        if event.content and event.content.parts:\n            print(event.content.parts[0].text)\n\n    await store.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"adapters/langchain/","title":"LangChain Adapter","text":"<p>pgmemory provides a LangChain-compatible memory class and agent tools.</p>"},{"location":"adapters/langchain/#installation","title":"Installation","text":"<p>No extra dependencies needed \u2014 just install <code>langchain-core</code> in your project alongside pgmemory:</p> <pre><code>pip install pgmemory langchain-core\n</code></pre>"},{"location":"adapters/langchain/#langchainmemory","title":"LangChainMemory","text":"<p><code>LangChainMemory</code> wraps pgmemory as a LangChain-compatible message history interface.</p> <pre><code>from pgmemory import MemoryStore, OpenAIEmbeddingProvider\nfrom pgmemory.adapters.langchain import LangChainMemory\n\nstore = MemoryStore(\n    \"postgresql+asyncpg://user:pass@localhost/mydb\",\n    OpenAIEmbeddingProvider(),\n)\nawait store.init()\n\nmemory = LangChainMemory(\n    store,\n    app_name=\"my_app\",\n    user_id=\"user_1\",\n    session_id=\"sess_abc\",   # optional, for provenance tracking\n    search_top_k=10,         # default: 10\n)\n</code></pre>"},{"location":"adapters/langchain/#async-usage-recommended","title":"Async usage (recommended)","text":"<pre><code># Add a message\nawait memory.aadd_message(\"User mentioned they prefer Python over Java\")\n\n# Search memories\nresults = await memory.asearch(\"programming language preferences\")\nfor r in results:\n    print(f\"[{r.memory.category.value}] {r.text} (sim={r.similarity})\")\n\n# Clear all memories for this user\ndeleted = await memory.aclear()\n</code></pre>"},{"location":"adapters/langchain/#sync-usage","title":"Sync usage","text":"<p>Sync wrappers are provided for LangChain's sync interface:</p> <pre><code>memory.add_message(\"User mentioned they prefer Python over Java\")\nresults = memory.search(\"programming language preferences\")\nmemory.clear()\n</code></pre> <p>Note</p> <p>Sync methods use <code>asyncio.get_event_loop().run_until_complete()</code> internally. If you're already in an async context, use the <code>a</code>-prefixed methods.</p>"},{"location":"adapters/langchain/#agent-tools","title":"Agent tools","text":"<p><code>build_langchain_tools()</code> creates LangChain <code>Tool</code> objects for agent use:</p> <pre><code>from pgmemory.adapters.langchain import build_langchain_tools\n\ntools = build_langchain_tools(store, \"my_app\", \"user_1\")\n</code></pre> Tool Description <code>search_memory</code> Search long-term memory for relevant past information <code>remember_fact</code> Store a fact in long-term memory <code>forget_memory</code> Expire a memory by its ID <p>All tools are async-native (<code>coroutine</code> parameter).</p>"},{"location":"adapters/langchain/#full-example","title":"Full example","text":"<pre><code>import asyncio\nfrom pgmemory import MemoryStore, OpenAIEmbeddingProvider\nfrom pgmemory.adapters.langchain import LangChainMemory, build_langchain_tools\n\nasync def main():\n    store = MemoryStore(\n        \"postgresql+asyncpg://mem_user:password@localhost/mem_db\",\n        OpenAIEmbeddingProvider(),\n    )\n    await store.init()\n\n    # As message history\n    memory = LangChainMemory(store, app_name=\"my_app\", user_id=\"user_1\")\n    await memory.aadd_message(\"I'm working on a data pipeline in Python\")\n    await memory.aadd_message(\"The deadline is end of March\")\n\n    results = await memory.asearch(\"current project\")\n    for r in results:\n        print(f\"{r.text} (score={r.combined_score})\")\n\n    # As agent tools\n    tools = build_langchain_tools(store, \"my_app\", \"user_1\")\n    # Pass `tools` to your LangChain agent\n\n    await store.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/adapters/","title":"Adapters","text":"<p>Framework adapters \u2014 thin wrappers that translate between your framework's interface and <code>MemoryStore</code>.</p>"},{"location":"api/adapters/#google-adk","title":"Google ADK","text":""},{"location":"api/adapters/#adkmemoryservice","title":"ADKMemoryService","text":""},{"location":"api/adapters/#pgmemory.adapters.adk.ADKMemoryService","title":"<code>pgmemory.adapters.adk.ADKMemoryService(store, *, search_top_k=10, search_threshold=0.4)</code>","text":"<p>ADK BaseMemoryService backed by pgmemory's MemoryStore.</p> <p>Implements the two required methods: - add_session_to_memory(session) - search_memory(app_name, user_id, query)</p> <p>Inherits from BaseMemoryService at runtime (so ADK recognises it) but is defined here to keep the import optional.</p>"},{"location":"api/adapters/#pgmemory.adapters.adk.ADKMemoryService.add_session_to_memory","title":"<code>add_session_to_memory(session)</code>  <code>async</code>","text":"<p>Ingest ADK Session events into pgmemory.</p> <p>Extracts text from each event, preserves provenance (session ID, event ID, timestamp, role), and batch-inserts.</p>"},{"location":"api/adapters/#pgmemory.adapters.adk.ADKMemoryService.search_memory","title":"<code>search_memory(*, app_name, user_id, query)</code>  <code>async</code>","text":"<p>Search pgmemory and return ADK SearchMemoryResponse.</p>"},{"location":"api/adapters/#build_adk_tools","title":"build_adk_tools","text":""},{"location":"api/adapters/#pgmemory.adapters.adk.build_adk_tools","title":"<code>pgmemory.adapters.adk.build_adk_tools(store)</code>","text":"<p>Create ADK FunctionTools bound to a MemoryStore.</p> <p>Returns tools for: search_memory, commit_session, remember_fact, forget, reinforce. Wire into your agent alongside ADK's preload_memory / load_memory.</p>"},{"location":"api/adapters/#langchain","title":"LangChain","text":""},{"location":"api/adapters/#langchainmemory","title":"LangChainMemory","text":""},{"location":"api/adapters/#pgmemory.adapters.langchain.LangChainMemory","title":"<code>pgmemory.adapters.langchain.LangChainMemory(store, app_name, user_id, *, session_id=None, search_top_k=10)</code>","text":"<p>LangChain BaseChatMessageHistory-compatible wrapper around pgmemory.</p> <p>Implements the interface LangChain expects: - add_message(message) - messages  (property) - clear()</p> <p>Also exposes search for retrieval-augmented generation.</p>"},{"location":"api/adapters/#pgmemory.adapters.langchain.LangChainMemory.aadd_message","title":"<code>aadd_message(message)</code>  <code>async</code>","text":"<p>Store a message as a memory. Returns memory ID.</p> <p>Accepts a string or any object with a .content attribute (e.g. LangChain BaseMessage).</p>"},{"location":"api/adapters/#pgmemory.adapters.langchain.LangChainMemory.asearch","title":"<code>asearch(query, **kwargs)</code>  <code>async</code>","text":"<p>Semantic + keyword hybrid search.</p>"},{"location":"api/adapters/#pgmemory.adapters.langchain.LangChainMemory.aclear","title":"<code>aclear()</code>  <code>async</code>","text":"<p>Delete all memories for this user. Returns count.</p>"},{"location":"api/adapters/#build_langchain_tools","title":"build_langchain_tools","text":""},{"location":"api/adapters/#pgmemory.adapters.langchain.build_langchain_tools","title":"<code>pgmemory.adapters.langchain.build_langchain_tools(store, app_name, user_id)</code>","text":"<p>Create LangChain Tool objects bound to a MemoryStore.</p> <p>Returns tools compatible with LangChain's agent framework.</p>"},{"location":"api/embeddings/","title":"Embeddings","text":"<p>Pluggable embedding providers. The store needs vectors \u2014 how you generate them is your business.</p>"},{"location":"api/embeddings/#embeddingprovider-base-class","title":"EmbeddingProvider (base class)","text":""},{"location":"api/embeddings/#pgmemory.embeddings.EmbeddingProvider","title":"<code>pgmemory.embeddings.EmbeddingProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base. Implement <code>embed</code> and <code>dimensions</code>.</p>"},{"location":"api/embeddings/#pgmemory.embeddings.EmbeddingProvider.dimensions","title":"<code>dimensions</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Dimensionality of the output vectors.</p>"},{"location":"api/embeddings/#pgmemory.embeddings.EmbeddingProvider.embed","title":"<code>embed(texts)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return one embedding vector per input text.</p>"},{"location":"api/embeddings/#vertexembeddingprovider","title":"VertexEmbeddingProvider","text":""},{"location":"api/embeddings/#pgmemory.embeddings.VertexEmbeddingProvider","title":"<code>pgmemory.embeddings.VertexEmbeddingProvider(model='text-embedding-004', *, task_type='RETRIEVAL_DOCUMENT', dimensionality=768)</code>","text":"<p>               Bases: <code>EmbeddingProvider</code></p> <p>Google Vertex AI text-embedding model.</p> <p>pip install pgmemory[vertex]</p>"},{"location":"api/embeddings/#ollamaembeddingprovider","title":"OllamaEmbeddingProvider","text":""},{"location":"api/embeddings/#pgmemory.embeddings.OllamaEmbeddingProvider","title":"<code>pgmemory.embeddings.OllamaEmbeddingProvider(model='nomic-embed-text', *, dimensionality=768, host=None)</code>","text":"<p>               Bases: <code>EmbeddingProvider</code></p> <p>Local Ollama (e.g. nomic-embed-text).</p> <p>pip install pgmemory[ollama]</p>"},{"location":"api/embeddings/#openaiembeddingprovider","title":"OpenAIEmbeddingProvider","text":""},{"location":"api/embeddings/#pgmemory.embeddings.OpenAIEmbeddingProvider","title":"<code>pgmemory.embeddings.OpenAIEmbeddingProvider(model='text-embedding-3-small', *, dimensionality=1536, api_key=None, base_url=None)</code>","text":"<p>               Bases: <code>EmbeddingProvider</code></p> <p>OpenAI / Azure OpenAI embeddings.</p> <p>pip install pgmemory[openai]</p>"},{"location":"api/store/","title":"MemoryStore","text":"<p>The core of pgmemory. All memory operations go through this class.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore","title":"<code>pgmemory.store.MemoryStore(connection_string, embedding_provider, *, table_name='memory', pool_size=5, pool_recycle=300, enrich_embeddings=True)</code>","text":"<p>Multi-user memory store backed by PostgreSQL + pgvector.</p> <p>One table. Hybrid search. Lifecycle management. That's it.</p> Usage <p>store = MemoryStore(     connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\",     embedding_provider=my_embedder, ) await store.init()</p>"},{"location":"api/store/#pgmemory.store.MemoryStore--store","title":"Store","text":"<p>mem_id = await store.add(\"my_app\", \"user_1\", \"User likes dark mode\",                           category=Category.PREFERENCE, importance=2)</p>"},{"location":"api/store/#pgmemory.store.MemoryStore--search-hybrid-keyword-semantic-recency","title":"Search (hybrid: keyword + semantic + recency)","text":"<p>results = await store.search(SearchQuery(     app_name=\"my_app\", user_id=\"user_1\", text=\"UI preferences\" ))</p>"},{"location":"api/store/#pgmemory.store.MemoryStore--lifecycle","title":"Lifecycle","text":"<p>await store.promote(mem_id) await store.expire(mem_id) await store.decay()</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Eager init: <code>async with MemoryStore(...) as store:</code>.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.init","title":"<code>init()</code>  <code>async</code>","text":"<p>Create the pgvector extension and memory table if needed.</p> <p>Call once at app startup, or let it auto-init on first operation.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Dispose of the connection pool.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.add","title":"<code>add(app_name, user_id, text_content, *, category=Category.GENERAL, importance=1, valid_until=None, source_session_id=None, source_event_id=None, source_event_timestamp=None, source_role=None, metadata=None)</code>  <code>async</code>","text":"<p>Store a single memory. Returns its ID.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.add_many","title":"<code>add_many(memories)</code>  <code>async</code>","text":"<p>Batch-insert multiple memories. Returns their IDs.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.search","title":"<code>search(query)</code>  <code>async</code>","text":"<p>Hybrid search: SQL filters \u2192 full-text + semantic + recency scoring.</p> <p>The pipeline: 1. Filter by app_name, user_id (always) 2. Filter by categories, min_importance, temporal validity (if set) 3. Score: weighted combination of cosine similarity, ts_rank, and recency 4. Sort by combined score, return top_k</p> Scoring formula <p>combined = (w_sim * cosine_similarity)          + (w_kw  * ts_rank)          + (w_rec * recency_decay)</p> <p>Where recency_decay = 1 / (1 + days_old / 30)</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.search_many","title":"<code>search_many(queries, *, top_k=10)</code>  <code>async</code>","text":"<p>Run multiple searches in parallel and merge into one ranked list.</p> <ul> <li>Fires all queries concurrently via asyncio.gather</li> <li>Deduplicates by memory.id, keeping the result with the highest combined_score</li> <li>Returns up to <code>top_k</code> results sorted by combined_score descending</li> </ul>"},{"location":"api/store/#pgmemory.store.MemoryStore.get","title":"<code>get(memory_id)</code>  <code>async</code>","text":"<p>Fetch a single memory by ID.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.promote","title":"<code>promote(memory_id, increment=1)</code>  <code>async</code>","text":"<p>Bump importance. Clears valid_until (reinforced = durable).</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.expire","title":"<code>expire(memory_id, *, reason='expired')</code>  <code>async</code>","text":"<p>Soft-expire a memory (set valid_until = now, log reason in metadata).</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.decay","title":"<code>decay(*, app_name=None, hard_delete=True)</code>  <code>async</code>","text":"<p>Remove memories past their valid_until.</p> <p>If hard_delete=False, does nothing (they're already filtered from search). Call from a scheduled job or let it run before each search.</p> <p>Returns number of rows deleted.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.soft_expire_stale","title":"<code>soft_expire_stale(*, max_age_days=90, min_importance=3, app_name=None)</code>  <code>async</code>","text":"<p>Set valid_until on old, low-importance memories that never expire.</p> <p>Memories with importance &gt;= min_importance are left alone. Returns number of rows updated.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.find_conflicts","title":"<code>find_conflicts(app_name, user_id, text_content, category, *, threshold=0.85, limit=5)</code>  <code>async</code>","text":"<p>Find active memories that are semantically close (potential conflicts).</p> <p>High similarity + same category + same user = likely duplicate or contradicting memory.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.supersede","title":"<code>supersede(app_name, user_id, new_text, category, *, threshold=0.85, **add_kwargs)</code>  <code>async</code>","text":"<p>Add a new memory and expire any conflicts it supersedes.</p> <p>Returns (new_memory_id, list_of_superseded_ids).</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.count","title":"<code>count(app_name=None, user_id=None, *, include_expired=False)</code>  <code>async</code>","text":"<p>Count memories, optionally filtered.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.delete_user","title":"<code>delete_user(app_name, user_id)</code>  <code>async</code>","text":"<p>Hard-delete all memories for a user. GDPR right-to-erasure.</p>"},{"location":"api/store/#pgmemory.store.MemoryStore.list_users","title":"<code>list_users(app_name)</code>  <code>async</code>","text":"<p>List distinct user_ids that have active memories in an app.</p>"},{"location":"api/types/","title":"Types","text":"<p>Core domain types \u2014 plain Python objects with no framework dependencies.</p>"},{"location":"api/types/#category","title":"Category","text":""},{"location":"api/types/#pgmemory.types.Category","title":"<code>pgmemory.types.Category</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>What kind of memory this is.</p> <p>Opinionated categories based on the Memori model. Every memory gets one. Use GENERAL as the catch-all when you don't know or don't care.</p>"},{"location":"api/types/#memory","title":"Memory","text":""},{"location":"api/types/#pgmemory.types.Memory","title":"<code>pgmemory.types.Memory(id=None, app_name='', user_id='', text='', category=Category.GENERAL, importance=1, created_at=(lambda: datetime.now(timezone.utc))(), valid_from=(lambda: datetime.now(timezone.utc))(), valid_until=None, last_accessed=None, source_session_id=None, source_event_id=None, source_event_timestamp=None, source_role=None, metadata=dict())</code>  <code>dataclass</code>","text":"<p>A single unit of remembered information.</p> <p>This is what you store, search for, and get back. Every memory belongs to one user within one app, has a category, and tracks its own lifecycle.</p>"},{"location":"api/types/#searchquery","title":"SearchQuery","text":""},{"location":"api/types/#pgmemory.types.SearchQuery","title":"<code>pgmemory.types.SearchQuery(app_name, user_id, text, categories=None, min_importance=None, include_expired=False, top_k=10, similarity_threshold=0.2, threshold_percentile=None, weight_similarity=0.6, weight_keyword=0.25, weight_recency=0.15)</code>  <code>dataclass</code>","text":"<p>Parameters for a memory search.</p> <p>The store will combine these into a hybrid SQL + semantic query.</p>"},{"location":"api/types/#searchresult","title":"SearchResult","text":""},{"location":"api/types/#pgmemory.types.SearchResult","title":"<code>pgmemory.types.SearchResult(memory, similarity=0.0, keyword_score=0.0, recency_score=0.0, combined_score=0.0, source_query=None)</code>  <code>dataclass</code>","text":"<p>A memory returned from search, enriched with relevance scoring.</p>"},{"location":"api/types/#pgmemory.types.SearchResult.format_results","title":"<code>format_results(results)</code>  <code>staticmethod</code>","text":"<p>Format search results as an LLM-readable string.</p>"}]}